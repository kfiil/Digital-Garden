{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Digital Garden \u00b6","title":"Digital Garden"},{"location":"#digital-garden","text":"","title":"Digital Garden"},{"location":"Decision%20Support%20Metrics/Decision%20Support%20Metrics/","text":"Decision Support Metrics \u00b6 When we define metrics, we need to ensure they are actionable and it is clear what kind of leadership decision we would make based on the metric. If the metric is not actionable and a change in it will not trigger a leadership decision, they are likely vanity metrics that provide little useful information. For each Decision Support Metrics we consider establishing, we should be able to answer: What action(s) would we take based on this metric? Content \u00b6 \u201cWhen a measure becomes a target, it ceases to be a good measure.\u201d Types of metrics (DevOps Handbook) Types of metrics Forming Your Error Budget Measure Productivity Related notes \u201cWhen a measure becomes a target, it ceases to be a good measure.\u201d \u00b6 Peter Drucker said \u201cif you can\u2019t measure it you can\u2019t improve it,\u201d but he didn\u2019t mention the second-order effects of that statement. What changes after people get used to the measurements? What if we measure things that are only partly relevant to what we\u2019re trying to improve? Tracking metrics can tell us something new, but can also create problems. When a measure becomes a target, it ceases to be a good measure. \u201cAny observed statistical regularity will tend to collapse once pressure is placed upon it for control purposes.\u201d Sources \u00b6 Podcast Complex systems & second-order effects The Changelog: Software Development, Open Source A New Morality of Attainment (Goodhart\u2019s Law) The Cobra Effect Redesigned (Examples & Antidotes) The Cobra Effect (Part 2) Uncertainty Saves Lives \u2013 the Peltzman Effect Four and More Types of Goodhart \u00b6 One of the best papers digging into variations of the metric and goal problem is Categorizing Variants of Goodhart\u2019s Law , by David Manheim and Scott Garrabrant. Their paper outlines four types of Goodhart\u2019s Law and why they happen. Regressional . When selecting a metric also selects a lot of noise. Example: choosing to do whatever the winners of \u201cperson of the year\u201d or \u201cbest company\u201d awards did. You might not see that the person was chosen to send a political message or that the company was manipulating numbers and will fall next year. Extremal . This comes from out-of-sample projections. When our initial information is within a specific boundary we may still want to project what could happen out of the boundary. In those extreme cases, the relationship between the metric and the goal may break down. Casual . Where the regulator (the intermediary between the proxy metric and the goal) causes the problem. For example, when pain became a 5th vital sign doctors were measured by their ability to make their patients more comfortable. If doctors start to prescribe pain medication too often or too easily they may increase addiction. Adversarial . Where agents have different goals from the regulator and find a loophole that harms the goal. For example, colonial powers wanting to decrease the number of cobras in India or rats in Vietnam and paying a bounty for dead cobras or rat tails. People discovered that they could raise their own cobras to kill or cut off rat tails and release the rats. This is known as the Cobra Effect . Types of metrics (DevOps Handbook) \u00b6 Business level \u00b6 Examples include the number of sales transactions, revenue of sales transactions, user signups, churn rate, A/B testing results, etc. Our goal is to have every business metric be actionable\u2014these top metrics should help inform how to change our product and be amenable to experimentation and A/B testing. When metrics aren\u2019t actionable, they are likely vanity metrics that provide little useful information\u2014these we want to store, but likely not display, let alone alert on. Application level \u00b6 Examples include transaction times, user response times, application faults, etc. Infrastructure level (e.g., database, operating system, networking, storage): Examples include web server traffic, CPU load, disk usage, etc. Client software level (e.g., JavaScript on the client browser, mobile application) \u00b6 Examples include application errors and crashes, user measured transaction times, etc. Deployment pipeline level \u00b6 Examples include build pipeline status (e.g., red or green for our various automated test suites), change deployment lead times, deployment frequencies, test environment promotions, and environment status. APPLICATION AND BUSINESS METRICS \u00b6 At the application level, our goal is to ensure that we are generating telemetry not only around application health (e.g., memory usage, transaction counts, etc.), but also to measure to what extent we are achieving our organizational goals (e.g., number of new users, user login events, user session lengths, percent of users active, how often certain features are being used, and so forth). For example, if we have a service that is supporting e-commerce, we want to ensure that we have telemetry around all of the user events that lead up to a successful transaction that generates revenue. We can then instrument all the user actions that are required for our desired customer outcomes. These metrics will vary according to different domains and organizational goals. For instance, for e-commerce sites, we may want to maximize the time spent on the site; however, for search engines, we may want to reduce the time spent on the site, since long sessions may indicate that users are having difficulty finding what they\u2019re looking for. In general, business metrics will be part of a customer acquisition funnel, which is the theoretical steps a potential customer will take to make a purchase. For instance, in an e-commerce site, the measurable journey events include total time on site, product link clicks, shopping cart adds, and completed orders. Our goal is to have every business metric be actionable\u2014these top metrics should help inform how to change our product and be amenable to experimentation and A/B testing. When metrics aren\u2019t actionable, they are likely vanity metrics that provide little useful information\u2014these we want to store, but likely not display, let alone alert on. Ideally, anyone viewing our information radiators will be able to make sense of the information we are showing in the context of desired organizational outcomes, such as goals around revenue, user attainment, conversion rates, etc. We should define and link each metric to a business outcome metric at the earliest stages of feature definition and development, and measure the outcomes after we deploy them in production. Furthermore, doing this helps product owners describe the business context of each feature for everyone in the value stream. By radiating how customers interact with what we build in the context of our goals, we enable fast feedback to feature teams so they can see whether the capabilities we are building are actually being used and to what extent they are achieving business goals. As a result, we reinforce the cultural expectations that instrumenting and analyzing customer usage is also a part of our daily work, so we better understand how our work contributes to our organizational goals. Types of metrics \u00b6 Throughput / productivity (Nr of deploys, nr of releases, WIP(open pull requests, tasks in doing etc), code review turnaround time, time to open pr, pr size, pr age, lines changed Distribution of Run, improve, Change, ) Software delivery performance (fx deployment frequency, lead time for changes, time to restore, change failure rate) Project / service / domain health (support tickets, log errors, CVE, availability, latency, throughput, error budgets) Squad / Lunar tech health (reviews, reviews across, eNPS) Forming Your Error Budget \u00b6 In order to base these decisions on objective data, the two teams jointly define a quarterly error budget based on the service\u2019s service level objective, or SLO (see Service Level Objectives ). The error budget provides a clear, objective metric that determines how unreliable the service is allowed to be within a single quarter. This metric removes the politics from negotiations between the SREs and the product developers when deciding how much risk to allow.Our practice is then as follows: Product Management defines an SLO, which sets an expectation of how much uptime the service should have per quarter. The actual uptime is measured by a neutral third party: our monitoring system. The difference between these two numbers is the \u201cbudget\u201d of how much \u201cunreliability\u201d is remaining for the quarter. As long as the uptime measured is above the SLO\u2014in other words, as long as there is error budget remaining\u2014new releases can be pushed. For example, imagine that a service\u2019s SLO is to successfully serve 99.999% of all queries per quarter. This means that the service\u2019s error budget is a failure rate of 0.001% for a given quarter. If a problem causes us to fail 0.0002% of the expected queries for the quarter, the problem spends 20% of the service\u2019s quarterly error budget. Measure Productivity \u00b6 CannotMeasureProductivity \u00b6 https://martinfowler.com/bliki/CannotMeasureProductivity.html Code diff a day \u00b6 The \u201cCode diff a day\u201d idea is that you on average should have created and landed a diff per day. This is a rule of thumb, so some days you might have less and some days you might have more. Thus the average part is important. Handling a diff a day carries a bunch of benefits: Cadence By aiming for a diff a day you get into a good rhythm of creating diffs, get them reviewed and landed every day. This also means that incrementally Uber will benefit from your code faster, as with a diff a day you make smaller steps and these get faster into production. Reviews With a diff a day you also ensure that your reviewers are happy, as a day worth of code tends to be fairly quickly reviewable (e.g., compared with a week\u2019s worth of code). When code changes are relatively small they tend to be faster reviewed, thus you are not blocked on code review for a long time. All in all, a win-win for you and your code reviewers.** Planning No matter if the feature I work on is big or small, the \u201ca code diff a day\u201d idea forces me to break it into pieces that I can do in a day. And these pieces also include tests and proper test coverage, thus when planning I include the time for testing and verification. Obviously, this idea can be misused. You could keep changing your favourite log statement to keep a high diff count, but such changes do not matter to Uber\u2019s perspective. Thus don\u2019t apply this idea naively. On the other hand, if you\u2019re in doubt, I would much rather get two smaller diffs to review (e.g., 50 - 100 lines), than one big (e.g., 100-200 lines). Note that these line counts include tests, though tests are typically easier to read and thus I would rather read 100 lines of test code, than 100 lines of logic. I tend to feel bad working on a diff where I have put more than a day\u2019s worth of coding into, as I know it will be harder (read: take longer) to review. Thus I try to plan such that every diff I send out, is for a single feature that takes at most a day to code. If it turns out I overestimated and that the diff only took e.g. half a day, then I\u2019ll still send out this diff for review. It also means that if I underestimated the effort I try to remove features/branches, to ensure that I stay within a day\u2019s worth of code in each diff. Small CLs: https://google.github.io/eng-practices/review/developer/small-cls.html How to do a code review: https://google.github.io/eng-practices/review/reviewer / Joakims test rant: https://github.com/recht/rants/blob/master/testing.md Related notes \u00b6 Project Health","title":"Decision Support Metrics"},{"location":"Decision%20Support%20Metrics/Decision%20Support%20Metrics/#decision-support-metrics","text":"When we define metrics, we need to ensure they are actionable and it is clear what kind of leadership decision we would make based on the metric. If the metric is not actionable and a change in it will not trigger a leadership decision, they are likely vanity metrics that provide little useful information. For each Decision Support Metrics we consider establishing, we should be able to answer: What action(s) would we take based on this metric?","title":"Decision Support Metrics"},{"location":"Decision%20Support%20Metrics/Decision%20Support%20Metrics/#content","text":"\u201cWhen a measure becomes a target, it ceases to be a good measure.\u201d Types of metrics (DevOps Handbook) Types of metrics Forming Your Error Budget Measure Productivity Related notes","title":"Content"},{"location":"Decision%20Support%20Metrics/Decision%20Support%20Metrics/#when-a-measure-becomes-a-target-it-ceases-to-be-a-good-measure","text":"Peter Drucker said \u201cif you can\u2019t measure it you can\u2019t improve it,\u201d but he didn\u2019t mention the second-order effects of that statement. What changes after people get used to the measurements? What if we measure things that are only partly relevant to what we\u2019re trying to improve? Tracking metrics can tell us something new, but can also create problems. When a measure becomes a target, it ceases to be a good measure. \u201cAny observed statistical regularity will tend to collapse once pressure is placed upon it for control purposes.\u201d","title":"\u201cWhen a measure becomes a target, it ceases to be a good measure.\u201d"},{"location":"Decision%20Support%20Metrics/Decision%20Support%20Metrics/#sources","text":"Podcast Complex systems & second-order effects The Changelog: Software Development, Open Source A New Morality of Attainment (Goodhart\u2019s Law) The Cobra Effect Redesigned (Examples & Antidotes) The Cobra Effect (Part 2) Uncertainty Saves Lives \u2013 the Peltzman Effect","title":"Sources"},{"location":"Decision%20Support%20Metrics/Decision%20Support%20Metrics/#four-and-more-types-of-goodhart","text":"One of the best papers digging into variations of the metric and goal problem is Categorizing Variants of Goodhart\u2019s Law , by David Manheim and Scott Garrabrant. Their paper outlines four types of Goodhart\u2019s Law and why they happen. Regressional . When selecting a metric also selects a lot of noise. Example: choosing to do whatever the winners of \u201cperson of the year\u201d or \u201cbest company\u201d awards did. You might not see that the person was chosen to send a political message or that the company was manipulating numbers and will fall next year. Extremal . This comes from out-of-sample projections. When our initial information is within a specific boundary we may still want to project what could happen out of the boundary. In those extreme cases, the relationship between the metric and the goal may break down. Casual . Where the regulator (the intermediary between the proxy metric and the goal) causes the problem. For example, when pain became a 5th vital sign doctors were measured by their ability to make their patients more comfortable. If doctors start to prescribe pain medication too often or too easily they may increase addiction. Adversarial . Where agents have different goals from the regulator and find a loophole that harms the goal. For example, colonial powers wanting to decrease the number of cobras in India or rats in Vietnam and paying a bounty for dead cobras or rat tails. People discovered that they could raise their own cobras to kill or cut off rat tails and release the rats. This is known as the Cobra Effect .","title":"Four and More Types of Goodhart"},{"location":"Decision%20Support%20Metrics/Decision%20Support%20Metrics/#types-of-metrics-devops-handbook","text":"","title":"Types of metrics (DevOps Handbook)"},{"location":"Decision%20Support%20Metrics/Decision%20Support%20Metrics/#business-level","text":"Examples include the number of sales transactions, revenue of sales transactions, user signups, churn rate, A/B testing results, etc. Our goal is to have every business metric be actionable\u2014these top metrics should help inform how to change our product and be amenable to experimentation and A/B testing. When metrics aren\u2019t actionable, they are likely vanity metrics that provide little useful information\u2014these we want to store, but likely not display, let alone alert on.","title":"Business level"},{"location":"Decision%20Support%20Metrics/Decision%20Support%20Metrics/#application-level","text":"Examples include transaction times, user response times, application faults, etc. Infrastructure level (e.g., database, operating system, networking, storage): Examples include web server traffic, CPU load, disk usage, etc.","title":"Application level"},{"location":"Decision%20Support%20Metrics/Decision%20Support%20Metrics/#client-software-level-eg-javascript-on-the-client-browser-mobile-application","text":"Examples include application errors and crashes, user measured transaction times, etc.","title":"Client software level (e.g., JavaScript on the client browser, mobile application)"},{"location":"Decision%20Support%20Metrics/Decision%20Support%20Metrics/#deployment-pipeline-level","text":"Examples include build pipeline status (e.g., red or green for our various automated test suites), change deployment lead times, deployment frequencies, test environment promotions, and environment status.","title":"Deployment pipeline level"},{"location":"Decision%20Support%20Metrics/Decision%20Support%20Metrics/#application-and-business-metrics","text":"At the application level, our goal is to ensure that we are generating telemetry not only around application health (e.g., memory usage, transaction counts, etc.), but also to measure to what extent we are achieving our organizational goals (e.g., number of new users, user login events, user session lengths, percent of users active, how often certain features are being used, and so forth). For example, if we have a service that is supporting e-commerce, we want to ensure that we have telemetry around all of the user events that lead up to a successful transaction that generates revenue. We can then instrument all the user actions that are required for our desired customer outcomes. These metrics will vary according to different domains and organizational goals. For instance, for e-commerce sites, we may want to maximize the time spent on the site; however, for search engines, we may want to reduce the time spent on the site, since long sessions may indicate that users are having difficulty finding what they\u2019re looking for. In general, business metrics will be part of a customer acquisition funnel, which is the theoretical steps a potential customer will take to make a purchase. For instance, in an e-commerce site, the measurable journey events include total time on site, product link clicks, shopping cart adds, and completed orders. Our goal is to have every business metric be actionable\u2014these top metrics should help inform how to change our product and be amenable to experimentation and A/B testing. When metrics aren\u2019t actionable, they are likely vanity metrics that provide little useful information\u2014these we want to store, but likely not display, let alone alert on. Ideally, anyone viewing our information radiators will be able to make sense of the information we are showing in the context of desired organizational outcomes, such as goals around revenue, user attainment, conversion rates, etc. We should define and link each metric to a business outcome metric at the earliest stages of feature definition and development, and measure the outcomes after we deploy them in production. Furthermore, doing this helps product owners describe the business context of each feature for everyone in the value stream. By radiating how customers interact with what we build in the context of our goals, we enable fast feedback to feature teams so they can see whether the capabilities we are building are actually being used and to what extent they are achieving business goals. As a result, we reinforce the cultural expectations that instrumenting and analyzing customer usage is also a part of our daily work, so we better understand how our work contributes to our organizational goals.","title":"APPLICATION AND BUSINESS METRICS"},{"location":"Decision%20Support%20Metrics/Decision%20Support%20Metrics/#types-of-metrics","text":"Throughput / productivity (Nr of deploys, nr of releases, WIP(open pull requests, tasks in doing etc), code review turnaround time, time to open pr, pr size, pr age, lines changed Distribution of Run, improve, Change, ) Software delivery performance (fx deployment frequency, lead time for changes, time to restore, change failure rate) Project / service / domain health (support tickets, log errors, CVE, availability, latency, throughput, error budgets) Squad / Lunar tech health (reviews, reviews across, eNPS)","title":"Types of metrics"},{"location":"Decision%20Support%20Metrics/Decision%20Support%20Metrics/#forming-your-error-budget","text":"In order to base these decisions on objective data, the two teams jointly define a quarterly error budget based on the service\u2019s service level objective, or SLO (see Service Level Objectives ). The error budget provides a clear, objective metric that determines how unreliable the service is allowed to be within a single quarter. This metric removes the politics from negotiations between the SREs and the product developers when deciding how much risk to allow.Our practice is then as follows: Product Management defines an SLO, which sets an expectation of how much uptime the service should have per quarter. The actual uptime is measured by a neutral third party: our monitoring system. The difference between these two numbers is the \u201cbudget\u201d of how much \u201cunreliability\u201d is remaining for the quarter. As long as the uptime measured is above the SLO\u2014in other words, as long as there is error budget remaining\u2014new releases can be pushed. For example, imagine that a service\u2019s SLO is to successfully serve 99.999% of all queries per quarter. This means that the service\u2019s error budget is a failure rate of 0.001% for a given quarter. If a problem causes us to fail 0.0002% of the expected queries for the quarter, the problem spends 20% of the service\u2019s quarterly error budget.","title":"Forming Your Error Budget"},{"location":"Decision%20Support%20Metrics/Decision%20Support%20Metrics/#measure-productivity","text":"","title":"Measure Productivity"},{"location":"Decision%20Support%20Metrics/Decision%20Support%20Metrics/#cannotmeasureproductivity","text":"https://martinfowler.com/bliki/CannotMeasureProductivity.html","title":"CannotMeasureProductivity"},{"location":"Decision%20Support%20Metrics/Decision%20Support%20Metrics/#code-diff-a-day","text":"The \u201cCode diff a day\u201d idea is that you on average should have created and landed a diff per day. This is a rule of thumb, so some days you might have less and some days you might have more. Thus the average part is important. Handling a diff a day carries a bunch of benefits: Cadence By aiming for a diff a day you get into a good rhythm of creating diffs, get them reviewed and landed every day. This also means that incrementally Uber will benefit from your code faster, as with a diff a day you make smaller steps and these get faster into production. Reviews With a diff a day you also ensure that your reviewers are happy, as a day worth of code tends to be fairly quickly reviewable (e.g., compared with a week\u2019s worth of code). When code changes are relatively small they tend to be faster reviewed, thus you are not blocked on code review for a long time. All in all, a win-win for you and your code reviewers.** Planning No matter if the feature I work on is big or small, the \u201ca code diff a day\u201d idea forces me to break it into pieces that I can do in a day. And these pieces also include tests and proper test coverage, thus when planning I include the time for testing and verification. Obviously, this idea can be misused. You could keep changing your favourite log statement to keep a high diff count, but such changes do not matter to Uber\u2019s perspective. Thus don\u2019t apply this idea naively. On the other hand, if you\u2019re in doubt, I would much rather get two smaller diffs to review (e.g., 50 - 100 lines), than one big (e.g., 100-200 lines). Note that these line counts include tests, though tests are typically easier to read and thus I would rather read 100 lines of test code, than 100 lines of logic. I tend to feel bad working on a diff where I have put more than a day\u2019s worth of coding into, as I know it will be harder (read: take longer) to review. Thus I try to plan such that every diff I send out, is for a single feature that takes at most a day to code. If it turns out I overestimated and that the diff only took e.g. half a day, then I\u2019ll still send out this diff for review. It also means that if I underestimated the effort I try to remove features/branches, to ensure that I stay within a day\u2019s worth of code in each diff. Small CLs: https://google.github.io/eng-practices/review/developer/small-cls.html How to do a code review: https://google.github.io/eng-practices/review/reviewer / Joakims test rant: https://github.com/recht/rants/blob/master/testing.md","title":"Code diff a day"},{"location":"Decision%20Support%20Metrics/Decision%20Support%20Metrics/#related-notes","text":"Project Health","title":"Related notes"},{"location":"Decision%20Support%20Metrics/Project%20Health/","text":"Project Health \u00b6 In order to give incentives to improve, the problematic parts needs to be measured and visualized in a good way. This page lists a set of metrics that are used to calculate which level a project is at. It is inspired by Google's old Test Certified definition and more recent developments internally at Google. Functional requirements \u00b6 See next section for numbers referred to as X. Level 1: Have a continuous build executing per commit Measure coverage on automated tests Level 2: Establishing a written policy that essentially forbids anyone from submitting untested code Require X% test coverage Max X static analysis errors Max execution time of all tests \\(with code coverage enabled\\) Level 3: Stricter limits of above metrics Minimum release frequency to production Level 4-5: Stricter limits of above metrics Notice: All criterias must be met to grade a project at a specific level. As soon one metric goes below, the level is reduced. Numerical requirements \u00b6 Level Test coverage Test execution time \\(all tests\\) SonarQube violations score \\(weighted\\) Release frequency \\(to prod\\) Release window allowed 1 >=40% <=60 min 2000 - - 2 >=50% <=20min 1000 - 02.00 - 05.00 CET 3 >=60% <=15min 500 <=1 month 02.00 - 05.00 CET 4 >=70% <=10min 200 <=2 weeks 02.00 - 05.00 CET, 06.00 - 08.00 CET, 14.00 - 16.00 CET 5 >=80% <=5min 100 <=1 week 02.00 - 05.00 CET, 06.00 - 08.00 CET, 14.00 - 16.00 CET SonarQube violations score \u00b6 In order to give incentives for fixing the more serious violations and to account for project size, the following formula is used for SonarQube violation points: score = \\(50 \\* blockers + 20 \\* critical + 5 \\* major + 1 \\* minor + 0.5 \\* info\\) / \\(NLOC/1000\\) Example : A project with 2000 lines of code with 3 blocker, 4 critical and 10 minor violations would get a score of \\(50\\*3+20\\*4+1\\*10\\) / \\(2000/1000\\) = 120 points. If the project only had 1000 lines of code, the score would be twice as high: 240. Potential future metrics \u00b6 Release branch cherry-picking: to measure the number of cherry-picks/hotfixes that are made to production releases \\(i.e. patch releases\\) . This depends on that release branches are cut from master and that cherry-picking takes place onto such branches for hotfixes. This is not the current release managment stategy for most projects, so it needs more work to be applicable. Categorizing tests as \"unit\", \"integration\" or \"end-to-end\" and have individual coverage requirements on each. There's one downside with this however - it gives incentives of writing too many integration and end-to-end tests, which changes the test balance away from the desired one, which should be according to the \"Test Pyramid\": Incremental test coverage : to measure coverage per Pull request, to ensure new code is tested. If the infrastructure can provide this number, this test coverage requirement should be higher than the total absolute coverage for a project. Test flakiness : to have a maximum allowed % of flaky tests \\(tests that fails sometimes, due to timing issues, poorly written code, network timeouts, etc\\) in each build. This requires an investment in infrastructure tooling in order to be possible to collect.","title":"Project Health"},{"location":"Decision%20Support%20Metrics/Project%20Health/#project-health","text":"In order to give incentives to improve, the problematic parts needs to be measured and visualized in a good way. This page lists a set of metrics that are used to calculate which level a project is at. It is inspired by Google's old Test Certified definition and more recent developments internally at Google.","title":"Project Health"},{"location":"Decision%20Support%20Metrics/Project%20Health/#functional-requirements","text":"See next section for numbers referred to as X. Level 1: Have a continuous build executing per commit Measure coverage on automated tests Level 2: Establishing a written policy that essentially forbids anyone from submitting untested code Require X% test coverage Max X static analysis errors Max execution time of all tests \\(with code coverage enabled\\) Level 3: Stricter limits of above metrics Minimum release frequency to production Level 4-5: Stricter limits of above metrics Notice: All criterias must be met to grade a project at a specific level. As soon one metric goes below, the level is reduced.","title":"Functional requirements"},{"location":"Decision%20Support%20Metrics/Project%20Health/#numerical-requirements","text":"Level Test coverage Test execution time \\(all tests\\) SonarQube violations score \\(weighted\\) Release frequency \\(to prod\\) Release window allowed 1 >=40% <=60 min 2000 - - 2 >=50% <=20min 1000 - 02.00 - 05.00 CET 3 >=60% <=15min 500 <=1 month 02.00 - 05.00 CET 4 >=70% <=10min 200 <=2 weeks 02.00 - 05.00 CET, 06.00 - 08.00 CET, 14.00 - 16.00 CET 5 >=80% <=5min 100 <=1 week 02.00 - 05.00 CET, 06.00 - 08.00 CET, 14.00 - 16.00 CET","title":"Numerical requirements"},{"location":"Decision%20Support%20Metrics/Project%20Health/#sonarqube-violations-score","text":"In order to give incentives for fixing the more serious violations and to account for project size, the following formula is used for SonarQube violation points: score = \\(50 \\* blockers + 20 \\* critical + 5 \\* major + 1 \\* minor + 0.5 \\* info\\) / \\(NLOC/1000\\) Example : A project with 2000 lines of code with 3 blocker, 4 critical and 10 minor violations would get a score of \\(50\\*3+20\\*4+1\\*10\\) / \\(2000/1000\\) = 120 points. If the project only had 1000 lines of code, the score would be twice as high: 240.","title":"SonarQube violations score"},{"location":"Decision%20Support%20Metrics/Project%20Health/#potential-future-metrics","text":"Release branch cherry-picking: to measure the number of cherry-picks/hotfixes that are made to production releases \\(i.e. patch releases\\) . This depends on that release branches are cut from master and that cherry-picking takes place onto such branches for hotfixes. This is not the current release managment stategy for most projects, so it needs more work to be applicable. Categorizing tests as \"unit\", \"integration\" or \"end-to-end\" and have individual coverage requirements on each. There's one downside with this however - it gives incentives of writing too many integration and end-to-end tests, which changes the test balance away from the desired one, which should be according to the \"Test Pyramid\": Incremental test coverage : to measure coverage per Pull request, to ensure new code is tested. If the infrastructure can provide this number, this test coverage requirement should be higher than the total absolute coverage for a project. Test flakiness : to have a maximum allowed % of flaky tests \\(tests that fails sometimes, due to timing issues, poorly written code, network timeouts, etc\\) in each build. This requires an investment in infrastructure tooling in order to be possible to collect.","title":"Potential future metrics"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Debugging%20Teams%20-%20Groundhog%20Day%20by%20Camille%20Fournier/","text":"# Debugging Teams: Groundhog Day - Camille Fournier \u00b6 Have you ever been on a team that seemed to work very hard but never move forward? Where you look back quarter after quarter, or perhaps year after year, and you did a lot, but nothing actually seemed to happen? Congratulations, you\u2019re in the middle of Groundhog Day. Groundhog Day (or perhaps Russian Doll, if you want a more modern reference), is the experience of doing plenty of work to move forward, and yet never quite seeing lasting change from that work. You feel busy but stuck in place. When you look back at the quarter, the past six months, or (most commonly) the year, and realize that you set a lot of goals, and even reached many of them, yet nothing feels different, you\u2019re stuck in Groundhog Day. It\u2019s all the same\u2026. The symptoms of Groundhog Day: \u00b6 The team is busy. The problem is not that the team isn\u2019t engaged. They\u2019re engaged. They are showing up and doing things every day. They may even be shipping features and completing projects. But somehow that work never seems to result in meaningful forward progress for the product or company. No one agrees on what the strategy is. Sure, the work the team is doing doesn\u2019t seem to change the business, but one thing does keep changing: the strategy. Depending on who you ask, when you ask, you will get a different answer. You don\u2019t really know where you\u2019re going, and you keep changing your mind. The solution is always just around the corner. Once we finish project X, things will be better. Once we sell to client Y, we\u2019ll be set. Once we hire key hire Z, things will really take off. It\u2019s no wonder things feel like Groundhog Day because no one has bothered to figure out what is wrong enough to dig in and fix it, and they are always betting the change will come from the next silver bullet. Groundhog Day is a common symptom, but one that can have several different causes, such as: \u00b6 A lack of long-term strategy. I guess you could see this one coming. If you really don\u2019t know where you\u2019re going, it\u2019s easy to end up in an extremely reactive situation where the goalposts keep moving. And without a clear and concrete strategy, no one feels comfortable building something big and meaningful. A strategy that changes constantly. Sometimes you see teams launch every quarter with a new big strategy. That big idea everyone was grinding away at last quarter is no longer important because leadership has decided that there\u2019s a new \u2018Most Important Thing\u2019 that we must drop everything to pursue. Without a clear and concrete strategy, no one feels comfortable building something big and meaningful. Too many different strategies at once. Sometimes this happens because there is a disagreement among leadership as to what should be done, and rather than resolving this conflict, each leader goes off in their own direction. Each product manager makes their own choices, engineers build features they think will be cool, and no one wants to have hard conversations and create strong alignment across the different parties. You don\u2019t lack a strategy, but you lack a single, agreed-upon strategy. Addicted to \u2018snacking\u2019. I recently came across the concept of \u2018snacking\u2019 in an old blog post by Intercom , which defines it as \u2018low-effort, low-impact work\u2019. Teams stuck in Groundhog Day mode often find themselves snacking, working on easy projects that seem like they should deliver quick wins but never actually make a difference. This can happen even when you do have a clear strategy, but it is most common when no one feels safe working on long-running projects because they fear that the work will be wasted after the next strategic change. A lack of strategic execution. I almost hate to separate execution from strategy, because far too many people think they can be strategic while lacking a plan for achieving their goals. Still, I\u2019ve seen many teams that have a big vision, but struggle to make a plan that will achieve their vision. There is always a reason why the vision hasn\u2019t been realized, but usually, it boils down to a lack of planning. The team may have a great end-goal, but they have no idea which steps will get them to achieve that end-goal, so they meander from project to project without making significant forward progress. A lack of metrics and accountability. Rarely, this happens because no one is actually measuring the work to see if any of the projects are actually making a difference against your goals. You don\u2019t learn anything project after project, so it\u2019s almost as if each project doesn\u2019t matter at all once it is completed. Without any metrics, you don\u2019t know what is working and what isn\u2019t working, and no one is held accountable for making sure that the work produced is actually meaningful. Failure to deal with the underlying circumstances surrounding the team. Maybe you have a great strategy, and a decent plan, but your team is constantly getting hung up on the unexpected. Too many outages from the old systems, and too many one-off customer requests. Or, your company is in turmoil, your team has incredible personnel turnover, and you just don\u2019t have enough people who even understand the team\u2019s context working to solve the problems. Again, the team is working hard, and they are shipping what they can with the bandwidth they have, but they don\u2019t have the ability to tackle the big stuff because they\u2019re just too distracted with other concerns. What to do? \u00b6 All of these situations have different detailed approaches that you will need to apply to get out of them, but they all start with the same action: slow down. You need to slow down, and maybe even stop if you want to fix this. Stop keeping yourself, and your team, too busy to think about what\u2019s important. The team needs to sit together and figure out a strategy that they actually believe in. Hold an off-site, and talk about where you want to be in a year or more. Take the time to write down your strategy and use it to set some longer-term goals. You may decide that you need to stop working on \u2018features\u2019 to invest in underlying foundational improvements that will let you do bigger things. Maybe those foundational improvements are stability-related, so you can actually focus on a feature for a period of time without firefighting. Maybe they are a rethinking of the architecture. Maybe the team needs to be restructured to work more effectively. But the only way to get out of a rut of sameness is to do something drastically different, and since your rut involves grinding out work, the way to get out of it is first to stop grinding. Slow down by asking why you\u2019re implementing a feature, and how you\u2019re going to know if it\u2019s successful, before you ship something. Slow down by looking at the results of that feature, and seeing if they matched what you were hoping to accomplish. Thinking of good metrics is hard, and the instrumentation you need to capture them can be a lot of work, but in the case of this team, you need to put these speed bumps in because the team needs to think about why they are building things and what is important. These questions and analyses force the team to do that work. It\u2019s hard to feel good about pumping the brakes when you\u2019re stressed out about a lack of forward progress, but getting out of this cycle requires you to strategize, plan, and decide \u2014 all of which are hard to do if you\u2019re too busy to even think.","title":"Debugging Teams: Groundhog Day - Camille Fournier"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Debugging%20Teams%20-%20Groundhog%20Day%20by%20Camille%20Fournier/#debugging-teams-groundhog-day-camille-fournier","text":"Have you ever been on a team that seemed to work very hard but never move forward? Where you look back quarter after quarter, or perhaps year after year, and you did a lot, but nothing actually seemed to happen? Congratulations, you\u2019re in the middle of Groundhog Day. Groundhog Day (or perhaps Russian Doll, if you want a more modern reference), is the experience of doing plenty of work to move forward, and yet never quite seeing lasting change from that work. You feel busy but stuck in place. When you look back at the quarter, the past six months, or (most commonly) the year, and realize that you set a lot of goals, and even reached many of them, yet nothing feels different, you\u2019re stuck in Groundhog Day. It\u2019s all the same\u2026.","title":"# Debugging Teams: Groundhog Day - Camille Fournier"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Debugging%20Teams%20-%20Groundhog%20Day%20by%20Camille%20Fournier/#the-symptoms-of-groundhog-day","text":"The team is busy. The problem is not that the team isn\u2019t engaged. They\u2019re engaged. They are showing up and doing things every day. They may even be shipping features and completing projects. But somehow that work never seems to result in meaningful forward progress for the product or company. No one agrees on what the strategy is. Sure, the work the team is doing doesn\u2019t seem to change the business, but one thing does keep changing: the strategy. Depending on who you ask, when you ask, you will get a different answer. You don\u2019t really know where you\u2019re going, and you keep changing your mind. The solution is always just around the corner. Once we finish project X, things will be better. Once we sell to client Y, we\u2019ll be set. Once we hire key hire Z, things will really take off. It\u2019s no wonder things feel like Groundhog Day because no one has bothered to figure out what is wrong enough to dig in and fix it, and they are always betting the change will come from the next silver bullet.","title":"The symptoms of Groundhog Day:"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Debugging%20Teams%20-%20Groundhog%20Day%20by%20Camille%20Fournier/#groundhog-day-is-a-common-symptom-but-one-that-can-have-several-different-causes-such-as","text":"A lack of long-term strategy. I guess you could see this one coming. If you really don\u2019t know where you\u2019re going, it\u2019s easy to end up in an extremely reactive situation where the goalposts keep moving. And without a clear and concrete strategy, no one feels comfortable building something big and meaningful. A strategy that changes constantly. Sometimes you see teams launch every quarter with a new big strategy. That big idea everyone was grinding away at last quarter is no longer important because leadership has decided that there\u2019s a new \u2018Most Important Thing\u2019 that we must drop everything to pursue. Without a clear and concrete strategy, no one feels comfortable building something big and meaningful. Too many different strategies at once. Sometimes this happens because there is a disagreement among leadership as to what should be done, and rather than resolving this conflict, each leader goes off in their own direction. Each product manager makes their own choices, engineers build features they think will be cool, and no one wants to have hard conversations and create strong alignment across the different parties. You don\u2019t lack a strategy, but you lack a single, agreed-upon strategy. Addicted to \u2018snacking\u2019. I recently came across the concept of \u2018snacking\u2019 in an old blog post by Intercom , which defines it as \u2018low-effort, low-impact work\u2019. Teams stuck in Groundhog Day mode often find themselves snacking, working on easy projects that seem like they should deliver quick wins but never actually make a difference. This can happen even when you do have a clear strategy, but it is most common when no one feels safe working on long-running projects because they fear that the work will be wasted after the next strategic change. A lack of strategic execution. I almost hate to separate execution from strategy, because far too many people think they can be strategic while lacking a plan for achieving their goals. Still, I\u2019ve seen many teams that have a big vision, but struggle to make a plan that will achieve their vision. There is always a reason why the vision hasn\u2019t been realized, but usually, it boils down to a lack of planning. The team may have a great end-goal, but they have no idea which steps will get them to achieve that end-goal, so they meander from project to project without making significant forward progress. A lack of metrics and accountability. Rarely, this happens because no one is actually measuring the work to see if any of the projects are actually making a difference against your goals. You don\u2019t learn anything project after project, so it\u2019s almost as if each project doesn\u2019t matter at all once it is completed. Without any metrics, you don\u2019t know what is working and what isn\u2019t working, and no one is held accountable for making sure that the work produced is actually meaningful. Failure to deal with the underlying circumstances surrounding the team. Maybe you have a great strategy, and a decent plan, but your team is constantly getting hung up on the unexpected. Too many outages from the old systems, and too many one-off customer requests. Or, your company is in turmoil, your team has incredible personnel turnover, and you just don\u2019t have enough people who even understand the team\u2019s context working to solve the problems. Again, the team is working hard, and they are shipping what they can with the bandwidth they have, but they don\u2019t have the ability to tackle the big stuff because they\u2019re just too distracted with other concerns.","title":"Groundhog Day is a common symptom, but one that can have several different causes, such as:"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Debugging%20Teams%20-%20Groundhog%20Day%20by%20Camille%20Fournier/#what-to-do","text":"All of these situations have different detailed approaches that you will need to apply to get out of them, but they all start with the same action: slow down. You need to slow down, and maybe even stop if you want to fix this. Stop keeping yourself, and your team, too busy to think about what\u2019s important. The team needs to sit together and figure out a strategy that they actually believe in. Hold an off-site, and talk about where you want to be in a year or more. Take the time to write down your strategy and use it to set some longer-term goals. You may decide that you need to stop working on \u2018features\u2019 to invest in underlying foundational improvements that will let you do bigger things. Maybe those foundational improvements are stability-related, so you can actually focus on a feature for a period of time without firefighting. Maybe they are a rethinking of the architecture. Maybe the team needs to be restructured to work more effectively. But the only way to get out of a rut of sameness is to do something drastically different, and since your rut involves grinding out work, the way to get out of it is first to stop grinding. Slow down by asking why you\u2019re implementing a feature, and how you\u2019re going to know if it\u2019s successful, before you ship something. Slow down by looking at the results of that feature, and seeing if they matched what you were hoping to accomplish. Thinking of good metrics is hard, and the instrumentation you need to capture them can be a lot of work, but in the case of this team, you need to put these speed bumps in because the team needs to think about why they are building things and what is important. These questions and analyses force the team to do that work. It\u2019s hard to feel good about pumping the brakes when you\u2019re stressed out about a lack of forward progress, but getting out of this cycle requires you to strategize, plan, and decide \u2014 all of which are hard to do if you\u2019re too busy to even think.","title":"What to do?"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Durable%20teams%20-%20The%20Managers%20Path%2C%20Camille%20Fournier/","text":"Durable teams are built on a shared purpose that comes from the company itself, and they align themselves with the company\u2019s values (see \u201cApplying Core Values\u201d in Chapter 9 for more on this topic). They have a clear understanding of the company\u2019s mission, and they see how their team fits into this mission. They can see that the mission requires many different types of teams, but all of the teams share a set of values. By creating a strong and enduring alignment between the team, its individuals, and the overall company, this purpose-based binding makes teams: Resilient to loss of individuals. While the clique is fragile, especially to the loss of the leader, the purpose-driven team tends to be very resilient to the loss of individuals and leadership. Because they\u2019re loyal to the mission of the larger organization, they can see a path forward even through loss. Driven to find better ways to achieve their purpose . Purpose-driven teams are more open to new ideas and value changes that can help them serve their purpose better. They care less about the source of an idea than its merit in achieving their goals. The members of these teams are interested in learning from others outside their function, and they actively seek out chances to collaborate more broadly to create the best results. First-team focused. Leaders who are strong team players understand that the people who report to them are not their first team. Instead, their first team is their peers across the company. This first-team focus helps them make decisions that consider the needs of the company as a whole before focusing on the needs of their team. Open to changes that serve their purpose. The collaborative leader understands that changes will happen to serve the wider purpose. Teams will change structure and people will need to move to where the business needs are. With that knowledge, these leaders create teams that are more flexible and understanding of frequent change in service of the larger vision. Getting clarity about the purpose of your team and your company can take time. In startups especially, there is often some confusion about the current goals and even sometimes the underlying mission. In the case where the goals are fuzzy and the mission is unclear, do your best to understand the company culture and think about how you can set your teams up to work well within that culture. By collaborating across teams and across business functions, your teams will come to understand the bigger picture and appreciate their mission as part of that picture. \u2014 The Manager's Path: A Guide for Tech Leaders Navigating Growth and Change by Camille Fournier https://a.co/aV3VcTe","title":"Durable teams   The Managers Path, Camille Fournier"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Guiding%20critical%20projects%20without%20micromanaging/","text":"Guiding critical projects without micromanaging \u00b6 The limits of flexible management \u00b6 I\u2019m a big believer in flexibility as a senior manager. I do not think I know or even can know the exact way to run any given team in my organization. The magic of effective teams is a combination of the personalities involved, the project lifecycle they\u2019re in, and so many other factors. Forcing every team into a single process, whether it\u2019s classic Agile-type two week sprints or Scrum or whatever is optimizing for uniformity of process at the likely expense of the needs of the teams themselves. Being outcomes-driven (is the work getting done, with good quality, in reasonable time, without burning out the people involved) is the only way I know how to work. However, as a senior manager, at some point you can make it harder for your managers to succeed when you give them very little structure to work with. It\u2019s tempting to say \u201cI don\u2019t care how you do any of it as long as it gets done.\u201d But that doesn\u2019t help people figure out what is important to you, so they have to guess at what they share, when, and how. And worse, without any clear expectations at all you deny your managers a powerful tool to use with their own teams: the reward (or perhaps, threat) of attention from their senior manager. Creating some lightweight process \u00b6 This lesson took me a long time to learn. When I came into my current job, I had \u2014 for the first time \u2014 a team of seasoned managers reporting to me. Being both lazy and not interested in the details of tracking work, I let these managers largely operate things in their own ways. We tracked OKRs and looked at project hits and misses every couple of weeks, but I stayed out of the details. And work got done. (Sometimes not as smoothly as I wanted, but I was busy with other things so I figured it was all ok.) The turning point came when my team got involved in a big, complex migration. The project was going ok, but there were a lot of moving parts. I trusted every person individually who was working on it, but I didn\u2019t feel like I understood the details and I was worried that we weren\u2019t asking ourselves the hard prioritization questions often enough. So I started a monthly update meeting. I know what you\u2019re thinking. A monthly status update meeting? What a waste of time! Why isn\u2019t it just a spreadsheet? And maybe I could\u2019ve managed this via a spreadsheet that I reviewed with one of the owners in our 1:1s, but the meeting ended up being a useful forcing function beyond any 1:1 check-in or asynchronous spreadsheet. What made this check-in meeting so valuable? \u00b6 First of all, this was a chance for discussion. I got to ask hard questions, and the team leadership got to show off. The team was forced to reach some agreement on the status before showing it to me, and my questions could reveal disagreements that they may not have resolved fully. They had a target every month that required preparation, coordination, and thought. And my presence was good for all of us, because it forced a group that didn\u2019t all share reporting lines below me to get on the same page, and gave each the opportunity to highlight disagreements with the others in real-time when they didn\u2019t feel aligned. Writing this all sounds very obvious. And once I started doing it, I realized that this was something I had been missing with many of my teams, for years. In an attempt to not micromanage, I had denied teams the opportunity to show off, to air grievances, and the forcing function they needed to get past their disagreements and get organized. When is this process useful? \u00b6 The outcome for that project was so positive, I expanded it to a few other areas in my organization. Running this process across different types of teams and projects, I can see its pluses and minuses. I now recommend this type of check-in for situations where any of the following apply: You have a critical area that has some misalignment between the participants. This can happen when there\u2019s a disagreement across product, engineering management, and/or the tech leads, or among partner teams working on a project. You have a manager who isn\u2019t getting into the details enough and needs some forcing function to get themselves (and their team) organized. They should not only just have status updates, but have status updates that they can explain every month. You have a strategic area where there is some uncertainty about where you should be going. You\u2019re learning new information month over month that can change the project focus and direction, and you need to hear about project status, but also how the team is taking in new information to inform future work. This type of meeting isn\u2019t necessary in an area that is running smoothly with a fairly stable roadmap and strong alignment across team members. You want to give these teams the chance to show off their accomplishments for you, but forcing a status meeting monthly is not the right format for that. These meetings also need to be adjusted or canceled once their reason for beginning is no longer there. When the misaligned participants get on the same page, the spreadsheet and quarterly update might be enough. When the manager is in the details and organized, you can now have the confidence that things are moving without needing to meet with a large group. When the strategic area gets clarity on their roadmap, there is no need to just talk about status updates. Do not keep these going forever just because you started them. Once they start to feel boring and rote, the meeting has outlasted its purpose. Cancel it, or reschedule it to happen less frequently. So if you happen to share my particular style of management \u2014 heavy on delegation, trust, and lightweight check-ins \u2014 I encourage you to add these forcing functions to your rotation. Give your teams the chance to show off, give your managers a boogeywoman to point to when they need the group to focus, and make life better for everyone. You owe it to them.","title":"Guiding critical projects without micromanaging"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Guiding%20critical%20projects%20without%20micromanaging/#guiding-critical-projects-without-micromanaging","text":"","title":"Guiding critical projects without micromanaging"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Guiding%20critical%20projects%20without%20micromanaging/#the-limits-of-flexible-management","text":"I\u2019m a big believer in flexibility as a senior manager. I do not think I know or even can know the exact way to run any given team in my organization. The magic of effective teams is a combination of the personalities involved, the project lifecycle they\u2019re in, and so many other factors. Forcing every team into a single process, whether it\u2019s classic Agile-type two week sprints or Scrum or whatever is optimizing for uniformity of process at the likely expense of the needs of the teams themselves. Being outcomes-driven (is the work getting done, with good quality, in reasonable time, without burning out the people involved) is the only way I know how to work. However, as a senior manager, at some point you can make it harder for your managers to succeed when you give them very little structure to work with. It\u2019s tempting to say \u201cI don\u2019t care how you do any of it as long as it gets done.\u201d But that doesn\u2019t help people figure out what is important to you, so they have to guess at what they share, when, and how. And worse, without any clear expectations at all you deny your managers a powerful tool to use with their own teams: the reward (or perhaps, threat) of attention from their senior manager.","title":"The limits of flexible management"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Guiding%20critical%20projects%20without%20micromanaging/#creating-some-lightweight-process","text":"This lesson took me a long time to learn. When I came into my current job, I had \u2014 for the first time \u2014 a team of seasoned managers reporting to me. Being both lazy and not interested in the details of tracking work, I let these managers largely operate things in their own ways. We tracked OKRs and looked at project hits and misses every couple of weeks, but I stayed out of the details. And work got done. (Sometimes not as smoothly as I wanted, but I was busy with other things so I figured it was all ok.) The turning point came when my team got involved in a big, complex migration. The project was going ok, but there were a lot of moving parts. I trusted every person individually who was working on it, but I didn\u2019t feel like I understood the details and I was worried that we weren\u2019t asking ourselves the hard prioritization questions often enough. So I started a monthly update meeting. I know what you\u2019re thinking. A monthly status update meeting? What a waste of time! Why isn\u2019t it just a spreadsheet? And maybe I could\u2019ve managed this via a spreadsheet that I reviewed with one of the owners in our 1:1s, but the meeting ended up being a useful forcing function beyond any 1:1 check-in or asynchronous spreadsheet.","title":"Creating some lightweight process"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Guiding%20critical%20projects%20without%20micromanaging/#what-made-this-check-in-meeting-so-valuable","text":"First of all, this was a chance for discussion. I got to ask hard questions, and the team leadership got to show off. The team was forced to reach some agreement on the status before showing it to me, and my questions could reveal disagreements that they may not have resolved fully. They had a target every month that required preparation, coordination, and thought. And my presence was good for all of us, because it forced a group that didn\u2019t all share reporting lines below me to get on the same page, and gave each the opportunity to highlight disagreements with the others in real-time when they didn\u2019t feel aligned. Writing this all sounds very obvious. And once I started doing it, I realized that this was something I had been missing with many of my teams, for years. In an attempt to not micromanage, I had denied teams the opportunity to show off, to air grievances, and the forcing function they needed to get past their disagreements and get organized.","title":"What made this check-in meeting so valuable?"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Guiding%20critical%20projects%20without%20micromanaging/#when-is-this-process-useful","text":"The outcome for that project was so positive, I expanded it to a few other areas in my organization. Running this process across different types of teams and projects, I can see its pluses and minuses. I now recommend this type of check-in for situations where any of the following apply: You have a critical area that has some misalignment between the participants. This can happen when there\u2019s a disagreement across product, engineering management, and/or the tech leads, or among partner teams working on a project. You have a manager who isn\u2019t getting into the details enough and needs some forcing function to get themselves (and their team) organized. They should not only just have status updates, but have status updates that they can explain every month. You have a strategic area where there is some uncertainty about where you should be going. You\u2019re learning new information month over month that can change the project focus and direction, and you need to hear about project status, but also how the team is taking in new information to inform future work. This type of meeting isn\u2019t necessary in an area that is running smoothly with a fairly stable roadmap and strong alignment across team members. You want to give these teams the chance to show off their accomplishments for you, but forcing a status meeting monthly is not the right format for that. These meetings also need to be adjusted or canceled once their reason for beginning is no longer there. When the misaligned participants get on the same page, the spreadsheet and quarterly update might be enough. When the manager is in the details and organized, you can now have the confidence that things are moving without needing to meet with a large group. When the strategic area gets clarity on their roadmap, there is no need to just talk about status updates. Do not keep these going forever just because you started them. Once they start to feel boring and rote, the meeting has outlasted its purpose. Cancel it, or reschedule it to happen less frequently. So if you happen to share my particular style of management \u2014 heavy on delegation, trust, and lightweight check-ins \u2014 I encourage you to add these forcing functions to your rotation. Give your teams the chance to show off, give your managers a boogeywoman to point to when they need the group to focus, and make life better for everyone. You owe it to them.","title":"When is this process useful?"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Strategy%20and%20Roadmap%20Uncertainty/","text":"Strategy and Roadmap Uncertainty \u00b6 Content \u00b6 Strategy Domain Roadmap High-performing Team Concerns Notes Strategy \u00b6 Mapping out domain landscape ( Domain Discovery & Core Domain Chart ) Make an investment decision - which of the proposed improvements to increasing differentiation or removing complexity should you choose? Domain \u00b6 Domain Discovery - A primary purpose of domain discovery techniques, like big picture event storming, is to map out the current state of the landscape, capturing concepts like business processes, socio-technical structures, and roles like customers and colleagues. Core Domain Chart - exploring possible evolutions. The purpose of this activity is to visualize options, moving from discovery towards action Business processes / Journey Overview (exisiting and know future ones) Roadmap \u00b6 Setting Expectations and Delivering on Schedule \u00b6 Notes here Strategies for Handling Roadmap Uncertainty \u00b6 Think about how to break down big projects into a series of smaller deliverables so that you can achieve some of the results, even if you don\u2019t necessarily complete the grand vision. Notes here Solution Approach \u00b6 Building the right thing, building the thing right Solution design(user story mapping, event storms) Risk assesment of new initiatives The Software Delivery Mantra Don\u2019t confuse motion with progress Don\u2019t confuse progress with the ability to deliver on time Don\u2019t confuse delivering on time with delivery of the right outcome Guiding critical projects \u00b6 Guiding Critical Projects Without Micromanaging - Being outcomes-driven (is the work getting done, with good quality, in reasonable time, without burning out the people involved) is the only way I know how to work. - I trusted every person individually who was working on it, but I didn\u2019t feel like I understood the details and I was worried that we weren\u2019t asking ourselves the hard prioritization questions often enough. So I started a monthly update meeting. - Manage Colony Leadership Team The colony leadership team and the squads follow up on the objectives in biweekly meetings High-performing Team \u00b6 Durable teams - The Managers Path, Camille Fournier Debugging Teams - Groundhog Day by Camille Fournier Questions How do you know you are doing a good job? How do you know you are doing the right thing? How do you know whether your colleagues need your help? Notes \u00b6 Setting Expectations and Delivering on Schedule \u00b6 One of the most frustrating questions that engineering managers get asked regularly is why something is taking so long. We\u2019ve all been asked this question before. We\u2019ve been asked it as hands-on engineers, as tech leads, and as managers of small teams, but the question takes on a whole new level of intensity when you\u2019re managing team managers, because answering it is significantly harder when you aren\u2019t embedded deeply in the details. First things first: hopefully you\u2019re being asked this question because something is running over plan by a significant margin. That is the time when it makes the most sense to ask, and when you should do your best to understand the scenario and answer. Sadly, we are often asked this question in times when things are not taking any longer than the estimate. We are often asked this question when our leadership, for whatever reason, either didn\u2019t like the original estimate or didn\u2019t ask for it at all, and now they\u2019re upset, despite nothing going wrong. Therefore, you must always be aggressive about sharing estimates and updates to estimates, even when people don\u2019t ask, especially if you believe that the project is critical or likely to take longer than a few weeks. This means you must be aggressive about getting estimates, and as we all know, software estimation is a very difficult process. Negotiating the process that your team uses to estimate, on what timescale, for what projects, may be part of your job at this level. Source 4. Resources/Books/The Manager's Path Strategies for Handling Roadmap Uncertainty \u00b6 There are few strategies I\u2019ve learned about building a roadmap: - Be realistic about the likelihood of changing plans given the size and stage of the company you work for. If your startup has a history of changing the year\u2019s plans every summer to account for the business results from the first half of the year, be prepared for a change in the summer, and try not to promise things to your team that would require continuity beyond that point. - Think about how to break down big projects into a series of smaller deliverables so that you can achieve some of the results, even if you don\u2019t necessarily complete the grand vision. Breaking down the technical work will require you to work closely with the product or business managers to figure out how the details should be prioritized. All of you should be aware by now that things will change quickly, so everything must be repeatedly reexamined with an eye toward what\u2019s most valuable right now. - Don\u2019t overpromise a future of technical projects. Don\u2019t promise your team exciting technical projects \u201clater,\u201d because the product roadmap for later hasn\u2019t been written yet. This kind of thinking will get hopes up and then disappoint. If the project is important, get it scheduled now \u2014 or as close to now as possible. If the project is not urgently important, you can put it on the backlog, but you should be realistic that once \u201clater\u201d rolls around, there will be a long list of competing priorities from other parts of the business. If you haven\u2019t taken the time to articulate the value of this work, it will get pushed aside in favor of projects that are more clearly valuable. - Dedicate 20% of your team\u2019s schedule to \u201csustaining engineering.\u201d This means allowing time for refactoring, fixing outstanding bugs, improving engineering processes, doing minor cleanup, and providing ongoing support. Take this into account in every planning session. Unfortunately, 20% is not enough to do big projects, so additional planning will be needed to get major technical rewrites or other big technical improvements. But without that 20% time, there will be negative consequences with missed delivery goals and unplanned and unpleasant cleanup work. - Understand how important various engineering projects really are. Product and business projects usually have some kind of value proposition to justify them. However, the same rigor isn\u2019t always applied when it comes to technical projects. When an engineer comes to you with an engineering project that she wants to do, think about framing the project by answering these questions: How big is that project? How important is it? Can you articulate the value of that project to anyone who asks? What would successful completion of the project mean for the team? The value of these questions is that you start to treat big technical projects the same way as product initiatives. These projects have advocates and goals, they have schedules, and they are managed like other big initiatives. This is a scary process because there are times when you \u201cknow\u201d something is important, but you don\u2019t know how to articulate it in a way that the business will value. Especially given the complex nature of technical projects and the challenge of measuring things like engineering efficiency, you\u2019re sometimes stuck trying to explain technical details to a nontechnical partner who may not totally understand where you\u2019re going or why. My advice is to do your best to gather data to support yourself, and talk about what will be possible when the work is done. If you look at a technical project and realize that you\u2019re proposing a bunch of work for a system that is rarely changed and won\u2019t enable core improvements to your technology or business, it probably isn\u2019t worth the effort. Unfortunately, there is never enough time for all the exploratory engineering, legacy code cleanup, and technical quality improvements your team will want to do, and this process will help you pick your battles. Source 4. Resources/Books/The Manager's Path The Software Delivery Mantra \u00b6 Navigating Complex Delivery \u00b6 Developing software with complex requirements or environments with ambiguity can make a team lose their heading. It requires structure to ensure accurate, timely delivery and everyone to be on the same page. In this article, I illustrate the steps I have cultivated in my career to make sure the course of the team is headed in the right direction when they veer off course. Credit: Dreamstime.com The Mantra \u00b6 \u2014 Don\u2019t confuse motion with progress \u2014 Don\u2019t confuse progress with the ability to deliver on time \u2014 Don\u2019t confuse delivering on time with delivery of the right outcome Motion is not always progress \u00b6 You could be treading water. The first area of inspection is generally looking to see if the activities of the team align to some direction. Things I look out for: - lack of clarity on the end goal - lack of smaller, near term milestones - a singular focus on activities in the absence of a plan Progress is great, how are we pacing? \u00b6 If the activities are driving incremental progress in the project, the next area is to see if the pace of work will result in delivery in a timely fashion. The pitfalls to watch out for here are: - lack of a work-back plan or an agile technique to estimate delivery (ie, story point heuristics) - lack of debate on tradeoffs on all sides of scope, quality and time - absence of communication and action taken on blockers (ie, clearly calling out dependencies) We are going to make it! Wait\u2026 where are we? \u00b6 This last one is most suited as a first step but when building software in an agile environment, there should be constant evaluation if the goals have changed since development started. Especially long lived initiatives or teams building out large platforms. Things to look out for: - lack of structured documentation outlining the goals - lack of community understanding of what the goal is (and explicitly what the goal is not) - absence of qualitative success metrics Summary \u00b6 While this article focused primarily on things to avoid, I hope it provides some cautionary signs to help you prevent your projects from heading to the wrong destination or never arriving. I have heard this phenomena termed \u201cSwirl\u201d also. Here is a bonus doodle:","title":"Strategy and Roadmap Uncertainty"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Strategy%20and%20Roadmap%20Uncertainty/#strategy-and-roadmap-uncertainty","text":"","title":"Strategy and Roadmap Uncertainty"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Strategy%20and%20Roadmap%20Uncertainty/#content","text":"Strategy Domain Roadmap High-performing Team Concerns Notes","title":"Content"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Strategy%20and%20Roadmap%20Uncertainty/#strategy","text":"Mapping out domain landscape ( Domain Discovery & Core Domain Chart ) Make an investment decision - which of the proposed improvements to increasing differentiation or removing complexity should you choose?","title":"Strategy"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Strategy%20and%20Roadmap%20Uncertainty/#domain","text":"Domain Discovery - A primary purpose of domain discovery techniques, like big picture event storming, is to map out the current state of the landscape, capturing concepts like business processes, socio-technical structures, and roles like customers and colleagues. Core Domain Chart - exploring possible evolutions. The purpose of this activity is to visualize options, moving from discovery towards action Business processes / Journey Overview (exisiting and know future ones)","title":"Domain"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Strategy%20and%20Roadmap%20Uncertainty/#roadmap","text":"","title":"Roadmap"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Strategy%20and%20Roadmap%20Uncertainty/#setting-expectations-and-delivering-on-schedule","text":"Notes here","title":"Setting Expectations and Delivering on Schedule"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Strategy%20and%20Roadmap%20Uncertainty/#strategies-for-handling-roadmap-uncertainty","text":"Think about how to break down big projects into a series of smaller deliverables so that you can achieve some of the results, even if you don\u2019t necessarily complete the grand vision. Notes here","title":"Strategies for Handling Roadmap Uncertainty"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Strategy%20and%20Roadmap%20Uncertainty/#solution-approach","text":"Building the right thing, building the thing right Solution design(user story mapping, event storms) Risk assesment of new initiatives The Software Delivery Mantra Don\u2019t confuse motion with progress Don\u2019t confuse progress with the ability to deliver on time Don\u2019t confuse delivering on time with delivery of the right outcome","title":"Solution Approach"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Strategy%20and%20Roadmap%20Uncertainty/#guiding-critical-projects","text":"Guiding Critical Projects Without Micromanaging - Being outcomes-driven (is the work getting done, with good quality, in reasonable time, without burning out the people involved) is the only way I know how to work. - I trusted every person individually who was working on it, but I didn\u2019t feel like I understood the details and I was worried that we weren\u2019t asking ourselves the hard prioritization questions often enough. So I started a monthly update meeting. - Manage Colony Leadership Team The colony leadership team and the squads follow up on the objectives in biweekly meetings","title":"Guiding critical projects"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Strategy%20and%20Roadmap%20Uncertainty/#high-performing-team","text":"Durable teams - The Managers Path, Camille Fournier Debugging Teams - Groundhog Day by Camille Fournier Questions How do you know you are doing a good job? How do you know you are doing the right thing? How do you know whether your colleagues need your help?","title":"High-performing Team"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Strategy%20and%20Roadmap%20Uncertainty/#notes","text":"","title":"Notes"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Strategy%20and%20Roadmap%20Uncertainty/#setting-expectations-and-delivering-on-schedule_1","text":"One of the most frustrating questions that engineering managers get asked regularly is why something is taking so long. We\u2019ve all been asked this question before. We\u2019ve been asked it as hands-on engineers, as tech leads, and as managers of small teams, but the question takes on a whole new level of intensity when you\u2019re managing team managers, because answering it is significantly harder when you aren\u2019t embedded deeply in the details. First things first: hopefully you\u2019re being asked this question because something is running over plan by a significant margin. That is the time when it makes the most sense to ask, and when you should do your best to understand the scenario and answer. Sadly, we are often asked this question in times when things are not taking any longer than the estimate. We are often asked this question when our leadership, for whatever reason, either didn\u2019t like the original estimate or didn\u2019t ask for it at all, and now they\u2019re upset, despite nothing going wrong. Therefore, you must always be aggressive about sharing estimates and updates to estimates, even when people don\u2019t ask, especially if you believe that the project is critical or likely to take longer than a few weeks. This means you must be aggressive about getting estimates, and as we all know, software estimation is a very difficult process. Negotiating the process that your team uses to estimate, on what timescale, for what projects, may be part of your job at this level. Source 4. Resources/Books/The Manager's Path","title":"Setting Expectations and Delivering on Schedule"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Strategy%20and%20Roadmap%20Uncertainty/#strategies-for-handling-roadmap-uncertainty_1","text":"There are few strategies I\u2019ve learned about building a roadmap: - Be realistic about the likelihood of changing plans given the size and stage of the company you work for. If your startup has a history of changing the year\u2019s plans every summer to account for the business results from the first half of the year, be prepared for a change in the summer, and try not to promise things to your team that would require continuity beyond that point. - Think about how to break down big projects into a series of smaller deliverables so that you can achieve some of the results, even if you don\u2019t necessarily complete the grand vision. Breaking down the technical work will require you to work closely with the product or business managers to figure out how the details should be prioritized. All of you should be aware by now that things will change quickly, so everything must be repeatedly reexamined with an eye toward what\u2019s most valuable right now. - Don\u2019t overpromise a future of technical projects. Don\u2019t promise your team exciting technical projects \u201clater,\u201d because the product roadmap for later hasn\u2019t been written yet. This kind of thinking will get hopes up and then disappoint. If the project is important, get it scheduled now \u2014 or as close to now as possible. If the project is not urgently important, you can put it on the backlog, but you should be realistic that once \u201clater\u201d rolls around, there will be a long list of competing priorities from other parts of the business. If you haven\u2019t taken the time to articulate the value of this work, it will get pushed aside in favor of projects that are more clearly valuable. - Dedicate 20% of your team\u2019s schedule to \u201csustaining engineering.\u201d This means allowing time for refactoring, fixing outstanding bugs, improving engineering processes, doing minor cleanup, and providing ongoing support. Take this into account in every planning session. Unfortunately, 20% is not enough to do big projects, so additional planning will be needed to get major technical rewrites or other big technical improvements. But without that 20% time, there will be negative consequences with missed delivery goals and unplanned and unpleasant cleanup work. - Understand how important various engineering projects really are. Product and business projects usually have some kind of value proposition to justify them. However, the same rigor isn\u2019t always applied when it comes to technical projects. When an engineer comes to you with an engineering project that she wants to do, think about framing the project by answering these questions: How big is that project? How important is it? Can you articulate the value of that project to anyone who asks? What would successful completion of the project mean for the team? The value of these questions is that you start to treat big technical projects the same way as product initiatives. These projects have advocates and goals, they have schedules, and they are managed like other big initiatives. This is a scary process because there are times when you \u201cknow\u201d something is important, but you don\u2019t know how to articulate it in a way that the business will value. Especially given the complex nature of technical projects and the challenge of measuring things like engineering efficiency, you\u2019re sometimes stuck trying to explain technical details to a nontechnical partner who may not totally understand where you\u2019re going or why. My advice is to do your best to gather data to support yourself, and talk about what will be possible when the work is done. If you look at a technical project and realize that you\u2019re proposing a bunch of work for a system that is rarely changed and won\u2019t enable core improvements to your technology or business, it probably isn\u2019t worth the effort. Unfortunately, there is never enough time for all the exploratory engineering, legacy code cleanup, and technical quality improvements your team will want to do, and this process will help you pick your battles. Source 4. Resources/Books/The Manager's Path","title":"Strategies for Handling Roadmap Uncertainty"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Strategy%20and%20Roadmap%20Uncertainty/#the-software-delivery-mantra","text":"","title":"The Software Delivery Mantra"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Strategy%20and%20Roadmap%20Uncertainty/#navigating-complex-delivery","text":"Developing software with complex requirements or environments with ambiguity can make a team lose their heading. It requires structure to ensure accurate, timely delivery and everyone to be on the same page. In this article, I illustrate the steps I have cultivated in my career to make sure the course of the team is headed in the right direction when they veer off course. Credit: Dreamstime.com","title":"Navigating Complex Delivery"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Strategy%20and%20Roadmap%20Uncertainty/#the-mantra","text":"\u2014 Don\u2019t confuse motion with progress \u2014 Don\u2019t confuse progress with the ability to deliver on time \u2014 Don\u2019t confuse delivering on time with delivery of the right outcome","title":"The Mantra"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Strategy%20and%20Roadmap%20Uncertainty/#motion-is-not-always-progress","text":"You could be treading water. The first area of inspection is generally looking to see if the activities of the team align to some direction. Things I look out for: - lack of clarity on the end goal - lack of smaller, near term milestones - a singular focus on activities in the absence of a plan","title":"Motion is not always progress"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Strategy%20and%20Roadmap%20Uncertainty/#progress-is-great-how-are-we-pacing","text":"If the activities are driving incremental progress in the project, the next area is to see if the pace of work will result in delivery in a timely fashion. The pitfalls to watch out for here are: - lack of a work-back plan or an agile technique to estimate delivery (ie, story point heuristics) - lack of debate on tradeoffs on all sides of scope, quality and time - absence of communication and action taken on blockers (ie, clearly calling out dependencies)","title":"Progress is great, how are we pacing?"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Strategy%20and%20Roadmap%20Uncertainty/#we-are-going-to-make-it-wait-where-are-we","text":"This last one is most suited as a first step but when building software in an agile environment, there should be constant evaluation if the goals have changed since development started. Especially long lived initiatives or teams building out large platforms. Things to look out for: - lack of structured documentation outlining the goals - lack of community understanding of what the goal is (and explicitly what the goal is not) - absence of qualitative success metrics","title":"We are going to make it! Wait\u2026 where are we?"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Strategy%20and%20Roadmap%20Uncertainty/#summary","text":"While this article focused primarily on things to avoid, I hope it provides some cautionary signs to help you prevent your projects from heading to the wrong destination or never arriving. I have heard this phenomena termed \u201cSwirl\u201d also. Here is a bonus doodle:","title":"Summary"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Turning%20Domain%20Discovery%20into%20Product%20and%20Organizational%20Improvements%20with%20a%20DDD%20Exemplar/","text":"Turning Domain Discovery into Product and Organizational Improvements with a DDD Exemplar \u00b6 One of the challenges I see regularly is inertia following domain discovery workshops. Techniques like big picture event storming are great for mapping out the business and visualising problems and opportunities, but that\u2019s where progress can easily get stuck. How do you go from event storm to product and organizational improvements? I recommend mapping out your domains on a core domain chart , exploring how you could evolve each domain, and creating a scorecard to decide which you should invest in. Often, companies want to combine improvements to their products with improvements to their operating model and ways of working. In my work, people usually want to introduce DDD practices into their ways of working. I recommend running a DDD Exemplar project. A DDD Exemplar is a project, ideally achievable within a quarter, that results in improvements in a domain(s) and improvements in how teams build products. The goal of an exemplar is to show how DDD can be applied within a specific organization. It lays the foundations for other teams to copy and adapt. It\u2019s a proof-of-concept for how teams build products. You don\u2019t have to call it a \u201cDDD Exemplar\u201d in your organization, you\u2019re welcome to use any name you prefer. Why Things Get Stuck After Discovery? \u00b6 In companies with many teams and many millions of lines of code, including a lot of legacy, there are endless things that can be improved in the product, software, and organization. Choosing what to focus on is a big decision and the wrong choice can have major implications for the business and the individual. People also worry about making the wrong decisions and having to abandon them. It\u2019s extremely frustrating as an engineer to work on something which is cancelled and to switch focus to something else. You expect the new project to be cancelled and it\u2019s hard to become motivated. Some technology leaders feel like they don\u2019t have the experience and skills to confidently lead major technology and organizational modernizations. They are used to managing on a smaller scale but the company has grown and they\u2019re outside their comfort zone. Another hurdle to decision making is when technology leaders feel that the skills needed to deliver big changes don\u2019t exist within the organization like the lack of DDD, architecture, or cloud skills. Choosing where to focus is a balancing act: delivering new products, fixing legacy software, and improving engineering culture. Different stakeholders usually assign different levels of importance to each of those needs, creating a conflict which also blocks progress on making a decision. If you are facing any of these challenges, I hope that the following suggestions can help you make progress. Evolving the Domain Landscape \u00b6 A primary purpose of domain discovery techniques, like big picture event storming, is to map out the current state of the landscape, capturing concepts like business processes, socio-technical structures, and roles like customers and colleagues. Any change you want to make to your product portfolio, internal or external- facing, is going to change your domain landscape. You\u2019re going to invest in one or more domains to make them more differentiating or reduce operating costs. After domain discovery, I recommend mapping out your domain landscape using a core domain chart , and exploring possible evolutions. The purpose of this activity is to visualize options, moving from discovery towards action. To create a core domain chart, start by placing each domain where you believe it currently lives on the chart. There are a few patterns which can help you. Measuring the differentiation and complexity of each domain is not trivial, so it\u2019s good to seek feedback from product and engineering people in each domain. ![[1 ZivHZQT9f4GCHJDIMIL6Jg.jpeg]] Start by mapping out the current state of your domains If you\u2019re not sure what your domains are, I have created a Strategic DDD Kata which provides a scenario for you to practice designing domain boundaries. You can apply the same techniques to your own landscape. Exploring Options \u00b6 When your domains are laid out on the core domain chart, you can begin to generate delivery options. For each domain chart on the chart, explore how you could evolve it by adding an arrow(s). Domains can be evolved generally in two directions. You can move them to the right of the chart to make them more differentiating. Usually, this is correlated with an increase in complexity, because to make a domain more differentiating you need to develop greater expertise and capabilities in that area. Domains can also be evolved downwards. This signifies removing complexity. The goal of removing complexity is usually to reduce the operating costs of a domain or improve the reliability. ![[Turning Domain Discovery into Product and Organizational Improvements with a DDD Exemplar 1.jpeg]] Creating options: exploring possible evolutions of your domain landscape In some domains, the existing software may be overly-complex and hard to evolve. You may want to show an arrow going down, and then an arrow moving to the right. This implies that you need to clean up the existing solution before you can add differentiating new capabilities. ![[Turning Domain Discovery into Product and Organizational Improvements with a DDD Exemplar 2.jpeg]] Sometimes you need to reduce unnecessary complexity before exploiting new complexity Complexity \u00b6 As you can see, there are different types of complexity. Some complexity we embrace because there is an opportunity to differentiate, whereas some complexity is providing little or no value and is simply making our systems more complex and expensive to maintain. It\u2019s also possible to exchange complexity. The example I see most often is complex manual processes ( Operational Complexity ) involving multiple people, excel spreadsheets, and a variety of tools. By automating these processes with software, the software gets more complex, but the manual processes become less complex. It can be tricky to capture the nuance of exchanging complexity on a core domain chart. It all depends on how you define the y-axis. I use Model Complexity , which means the effort required to learn the domain, model it in software, and maintain the software. This doesn\u2019t include the complexity of manual processes so I would show an arrow going up with a small note. ![[Turning Domain Discovery into Product and Organizational Improvements with a DDD Exemplar 3.jpeg]] Adding complexity to the software domain model to reduce operational complexity (e.g. manual processes) DDD Exemplar \u00b6 Mapping out your domain landscape provides options. The next step is to make an investment decision: which of the proposed improvements to increasing differentiation or removing complexity should you choose? I recommend creating a scorecard which scores each option against criteria that are most important to you. I have a separate post on this topic. One of the common scenarios I deal with is organizations who not only want to evolve their landscape but also improve their ways of working and DDD expertise. In these situations, I usually recommend a DDD exemplar. A DDD Exemplar is a project which will evolve the domain landscape in a meaningful way, while also upskilling a team with DDD techniques. The most important aspect of a DDD Exemplar is that it will provide the foundations of an organizational model that can be applied to other teams if successful. Choosing a DDD Exemplar \u00b6 When looking at the domain landscape for options that would be suitable as a DDD Exemplar, there is no specific rule on which to choose. It could be adding differentiation to a core domain or reducing complexity in a supporting domain. Here\u2019s one example of a domain evolution which was a good fit for a DDD Exemplar. There is a good balance of business, technology, and organizational improvements. Current process is highly manual involving 20+ people scattered across different teams. Big picture event storming can be used to map out the current process, and help to build a lasting collaborative approach between product, engineering, and other disciplines. The future automated process needs to be carefully designed, not just a a copy of the existing manual process. Another chance to use collaborative DDD techniques, this time for designing new processes. The domain has a number of intricate challenges which will provide the team with a great opportunity to practice and improve their software domain modelling skills. The domain is a supporting domain but is a key enabler of existing and future core domains. Removing the operational complexity would make it easier to support future changes in core domains. A meaningful outcome can be delivered within a quarter which would be satisfying for business stakeholders \u2014 the new process will free up a lot of time and result in less support cases. One piece of advice: choosing the right people for an exemplar is key. It\u2019s not a place for sceptical senior engineers who think they know everything, it is a place for people who like learning and working collaboratively, and sharing what they\u2019ve learned with other teams. If you like concepts in DDD but you\u2019re not sure how it all fits together, the DDD Starter Modelling Process is my recommended place to get started. It can guide you through your first few cycles of discovery, strategy, and implementation and then you\u2019ll be in a good place to guide yourself. Building the Narrative \u00b6 Building a narrative is important for at-least two reasons. The first reason is to sense-check your plans: does your proposal for a DDD exemplar make business sense, or is it driven by personal interest in the domains you are personally most interested in evolving? The second reason is that you may need to sell the idea to get buy-in and investment to run with it. I\u2019ve seen a lot of good ideas fail to get off the ground because they weren\u2019t pitched in the right way. My advice is to use Simon Wardley\u2019s Strategy Cycle as the basis for your narrative. You don\u2019t need to reference the cycle in your narrative, but I recommend loosely following the structure. ![[Turning Domain Discovery into Product and Organizational Improvements with a DDD Exemplar 4.jpg]] The Strategy Cycle. Credit: Simon Wardley Starting with purpose, what is your organization trying to achieve? What are the most important challenges or opportunities your company is facing? Two companies I recently spoken to were facing a similar challenge: they had made a major pivot in their business model and were forging a new industry. They had validated the concept and taken on a large amount of funding. While exploiting the new business models they also needed to scale their organizations. This is the type of business context I would use to begin the narrative. The second part of the narrative is the landscape. For this, you can use the core domain chart you created of the current state. You\u2019ll need to add details around the core domain chart but the diagram can summarize the current and possible future states. The third part of the narrative is the climate. How is your landscape evolving based on external pressures? Your core domain might be moving left because your competitor\u2019s have launched a new innovation that renders yours obsolete. The fourth part of the narrative is doctrine. This focuses on how your organization is operating. In your narrative for a DDD exemplar, you may want to highlight weaknesses in certain DDD practices, like collaborative design, that the organization must improve if it wants to achieve its purpose. When organizations scale, the costs of inefficiencies in how they work are amplified. I include this type of information to stress the importance of doctrine. The final part is leadership. This is the possible evolutions on your core domain chart and your proposal which may be a DDD exemplar or something else. I recommend that a good proposal outlines the why and talks a little bit about the how. How are you going to approach the problem? How are you going to put the right people in place? How are you going to measure success? I try to make it as easy as possible for management to realise it\u2019s a good idea, that has been well planned out so they can just say \u201cyes\u201d. In Reality\u2026 \u00b6 I\u2019m not advocating that you follow every step in this article in order and propose a DDD exemplar. Every company has a unique context of challenges, goals, and people that there is never simple a recipe for making good strategic decisions. What I wanted to emphasise in this article is that I see a lot of teams getting stuck after domain discovery and not knowing what to do next. I\u2019ve outlined the general approach I use: map out the domain landscape, identify possible evolutions, build a narrative, and use a DDD Exemplar if you\u2019re trying to introduce a DDD approach into your organization. I hope this approach can give you inspiration to help you break the post-discovery inertia. Please feel free to leave a comment or contact me directly if you have any questions or want to add to the discussion.","title":"Turning Domain Discovery into Product and Organizational Improvements with a DDD Exemplar"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Turning%20Domain%20Discovery%20into%20Product%20and%20Organizational%20Improvements%20with%20a%20DDD%20Exemplar/#turning-domain-discovery-into-product-and-organizational-improvements-with-a-ddd-exemplar","text":"One of the challenges I see regularly is inertia following domain discovery workshops. Techniques like big picture event storming are great for mapping out the business and visualising problems and opportunities, but that\u2019s where progress can easily get stuck. How do you go from event storm to product and organizational improvements? I recommend mapping out your domains on a core domain chart , exploring how you could evolve each domain, and creating a scorecard to decide which you should invest in. Often, companies want to combine improvements to their products with improvements to their operating model and ways of working. In my work, people usually want to introduce DDD practices into their ways of working. I recommend running a DDD Exemplar project. A DDD Exemplar is a project, ideally achievable within a quarter, that results in improvements in a domain(s) and improvements in how teams build products. The goal of an exemplar is to show how DDD can be applied within a specific organization. It lays the foundations for other teams to copy and adapt. It\u2019s a proof-of-concept for how teams build products. You don\u2019t have to call it a \u201cDDD Exemplar\u201d in your organization, you\u2019re welcome to use any name you prefer.","title":"Turning Domain Discovery into Product and Organizational Improvements with a DDD Exemplar"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Turning%20Domain%20Discovery%20into%20Product%20and%20Organizational%20Improvements%20with%20a%20DDD%20Exemplar/#why-things-get-stuck-after-discovery","text":"In companies with many teams and many millions of lines of code, including a lot of legacy, there are endless things that can be improved in the product, software, and organization. Choosing what to focus on is a big decision and the wrong choice can have major implications for the business and the individual. People also worry about making the wrong decisions and having to abandon them. It\u2019s extremely frustrating as an engineer to work on something which is cancelled and to switch focus to something else. You expect the new project to be cancelled and it\u2019s hard to become motivated. Some technology leaders feel like they don\u2019t have the experience and skills to confidently lead major technology and organizational modernizations. They are used to managing on a smaller scale but the company has grown and they\u2019re outside their comfort zone. Another hurdle to decision making is when technology leaders feel that the skills needed to deliver big changes don\u2019t exist within the organization like the lack of DDD, architecture, or cloud skills. Choosing where to focus is a balancing act: delivering new products, fixing legacy software, and improving engineering culture. Different stakeholders usually assign different levels of importance to each of those needs, creating a conflict which also blocks progress on making a decision. If you are facing any of these challenges, I hope that the following suggestions can help you make progress.","title":"Why Things Get Stuck After Discovery?"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Turning%20Domain%20Discovery%20into%20Product%20and%20Organizational%20Improvements%20with%20a%20DDD%20Exemplar/#evolving-the-domain-landscape","text":"A primary purpose of domain discovery techniques, like big picture event storming, is to map out the current state of the landscape, capturing concepts like business processes, socio-technical structures, and roles like customers and colleagues. Any change you want to make to your product portfolio, internal or external- facing, is going to change your domain landscape. You\u2019re going to invest in one or more domains to make them more differentiating or reduce operating costs. After domain discovery, I recommend mapping out your domain landscape using a core domain chart , and exploring possible evolutions. The purpose of this activity is to visualize options, moving from discovery towards action. To create a core domain chart, start by placing each domain where you believe it currently lives on the chart. There are a few patterns which can help you. Measuring the differentiation and complexity of each domain is not trivial, so it\u2019s good to seek feedback from product and engineering people in each domain. ![[1 ZivHZQT9f4GCHJDIMIL6Jg.jpeg]] Start by mapping out the current state of your domains If you\u2019re not sure what your domains are, I have created a Strategic DDD Kata which provides a scenario for you to practice designing domain boundaries. You can apply the same techniques to your own landscape.","title":"Evolving the Domain Landscape"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Turning%20Domain%20Discovery%20into%20Product%20and%20Organizational%20Improvements%20with%20a%20DDD%20Exemplar/#exploring-options","text":"When your domains are laid out on the core domain chart, you can begin to generate delivery options. For each domain chart on the chart, explore how you could evolve it by adding an arrow(s). Domains can be evolved generally in two directions. You can move them to the right of the chart to make them more differentiating. Usually, this is correlated with an increase in complexity, because to make a domain more differentiating you need to develop greater expertise and capabilities in that area. Domains can also be evolved downwards. This signifies removing complexity. The goal of removing complexity is usually to reduce the operating costs of a domain or improve the reliability. ![[Turning Domain Discovery into Product and Organizational Improvements with a DDD Exemplar 1.jpeg]] Creating options: exploring possible evolutions of your domain landscape In some domains, the existing software may be overly-complex and hard to evolve. You may want to show an arrow going down, and then an arrow moving to the right. This implies that you need to clean up the existing solution before you can add differentiating new capabilities. ![[Turning Domain Discovery into Product and Organizational Improvements with a DDD Exemplar 2.jpeg]] Sometimes you need to reduce unnecessary complexity before exploiting new complexity","title":"Exploring Options"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Turning%20Domain%20Discovery%20into%20Product%20and%20Organizational%20Improvements%20with%20a%20DDD%20Exemplar/#complexity","text":"As you can see, there are different types of complexity. Some complexity we embrace because there is an opportunity to differentiate, whereas some complexity is providing little or no value and is simply making our systems more complex and expensive to maintain. It\u2019s also possible to exchange complexity. The example I see most often is complex manual processes ( Operational Complexity ) involving multiple people, excel spreadsheets, and a variety of tools. By automating these processes with software, the software gets more complex, but the manual processes become less complex. It can be tricky to capture the nuance of exchanging complexity on a core domain chart. It all depends on how you define the y-axis. I use Model Complexity , which means the effort required to learn the domain, model it in software, and maintain the software. This doesn\u2019t include the complexity of manual processes so I would show an arrow going up with a small note. ![[Turning Domain Discovery into Product and Organizational Improvements with a DDD Exemplar 3.jpeg]] Adding complexity to the software domain model to reduce operational complexity (e.g. manual processes)","title":"Complexity"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Turning%20Domain%20Discovery%20into%20Product%20and%20Organizational%20Improvements%20with%20a%20DDD%20Exemplar/#ddd-exemplar","text":"Mapping out your domain landscape provides options. The next step is to make an investment decision: which of the proposed improvements to increasing differentiation or removing complexity should you choose? I recommend creating a scorecard which scores each option against criteria that are most important to you. I have a separate post on this topic. One of the common scenarios I deal with is organizations who not only want to evolve their landscape but also improve their ways of working and DDD expertise. In these situations, I usually recommend a DDD exemplar. A DDD Exemplar is a project which will evolve the domain landscape in a meaningful way, while also upskilling a team with DDD techniques. The most important aspect of a DDD Exemplar is that it will provide the foundations of an organizational model that can be applied to other teams if successful.","title":"DDD Exemplar"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Turning%20Domain%20Discovery%20into%20Product%20and%20Organizational%20Improvements%20with%20a%20DDD%20Exemplar/#choosing-a-ddd-exemplar","text":"When looking at the domain landscape for options that would be suitable as a DDD Exemplar, there is no specific rule on which to choose. It could be adding differentiation to a core domain or reducing complexity in a supporting domain. Here\u2019s one example of a domain evolution which was a good fit for a DDD Exemplar. There is a good balance of business, technology, and organizational improvements. Current process is highly manual involving 20+ people scattered across different teams. Big picture event storming can be used to map out the current process, and help to build a lasting collaborative approach between product, engineering, and other disciplines. The future automated process needs to be carefully designed, not just a a copy of the existing manual process. Another chance to use collaborative DDD techniques, this time for designing new processes. The domain has a number of intricate challenges which will provide the team with a great opportunity to practice and improve their software domain modelling skills. The domain is a supporting domain but is a key enabler of existing and future core domains. Removing the operational complexity would make it easier to support future changes in core domains. A meaningful outcome can be delivered within a quarter which would be satisfying for business stakeholders \u2014 the new process will free up a lot of time and result in less support cases. One piece of advice: choosing the right people for an exemplar is key. It\u2019s not a place for sceptical senior engineers who think they know everything, it is a place for people who like learning and working collaboratively, and sharing what they\u2019ve learned with other teams. If you like concepts in DDD but you\u2019re not sure how it all fits together, the DDD Starter Modelling Process is my recommended place to get started. It can guide you through your first few cycles of discovery, strategy, and implementation and then you\u2019ll be in a good place to guide yourself.","title":"Choosing a DDD Exemplar"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Turning%20Domain%20Discovery%20into%20Product%20and%20Organizational%20Improvements%20with%20a%20DDD%20Exemplar/#building-the-narrative","text":"Building a narrative is important for at-least two reasons. The first reason is to sense-check your plans: does your proposal for a DDD exemplar make business sense, or is it driven by personal interest in the domains you are personally most interested in evolving? The second reason is that you may need to sell the idea to get buy-in and investment to run with it. I\u2019ve seen a lot of good ideas fail to get off the ground because they weren\u2019t pitched in the right way. My advice is to use Simon Wardley\u2019s Strategy Cycle as the basis for your narrative. You don\u2019t need to reference the cycle in your narrative, but I recommend loosely following the structure. ![[Turning Domain Discovery into Product and Organizational Improvements with a DDD Exemplar 4.jpg]] The Strategy Cycle. Credit: Simon Wardley Starting with purpose, what is your organization trying to achieve? What are the most important challenges or opportunities your company is facing? Two companies I recently spoken to were facing a similar challenge: they had made a major pivot in their business model and were forging a new industry. They had validated the concept and taken on a large amount of funding. While exploiting the new business models they also needed to scale their organizations. This is the type of business context I would use to begin the narrative. The second part of the narrative is the landscape. For this, you can use the core domain chart you created of the current state. You\u2019ll need to add details around the core domain chart but the diagram can summarize the current and possible future states. The third part of the narrative is the climate. How is your landscape evolving based on external pressures? Your core domain might be moving left because your competitor\u2019s have launched a new innovation that renders yours obsolete. The fourth part of the narrative is doctrine. This focuses on how your organization is operating. In your narrative for a DDD exemplar, you may want to highlight weaknesses in certain DDD practices, like collaborative design, that the organization must improve if it wants to achieve its purpose. When organizations scale, the costs of inefficiencies in how they work are amplified. I include this type of information to stress the importance of doctrine. The final part is leadership. This is the possible evolutions on your core domain chart and your proposal which may be a DDD exemplar or something else. I recommend that a good proposal outlines the why and talks a little bit about the how. How are you going to approach the problem? How are you going to put the right people in place? How are you going to measure success? I try to make it as easy as possible for management to realise it\u2019s a good idea, that has been well planned out so they can just say \u201cyes\u201d.","title":"Building the Narrative"},{"location":"Strategy%20and%20Roadmap%20Uncertainty/Turning%20Domain%20Discovery%20into%20Product%20and%20Organizational%20Improvements%20with%20a%20DDD%20Exemplar/#in-reality","text":"I\u2019m not advocating that you follow every step in this article in order and propose a DDD exemplar. Every company has a unique context of challenges, goals, and people that there is never simple a recipe for making good strategic decisions. What I wanted to emphasise in this article is that I see a lot of teams getting stuck after domain discovery and not knowing what to do next. I\u2019ve outlined the general approach I use: map out the domain landscape, identify possible evolutions, build a narrative, and use a DDD Exemplar if you\u2019re trying to introduce a DDD approach into your organization. I hope this approach can give you inspiration to help you break the post-discovery inertia. Please feel free to leave a comment or contact me directly if you have any questions or want to add to the discussion.","title":"In Reality\u2026"}]}